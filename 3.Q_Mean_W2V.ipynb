{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRP-fAQedMTd"
   },
   "source": [
    "<h2> 3.6 Featurizing text data with tfidf weighted word-vectors </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1105,
     "status": "ok",
     "timestamp": 1644585091097,
     "user": {
      "displayName": "Applied AI Course",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSpfk3evOnQu4WaAr65Jsk94o0T_MicRlfD3-hAVs=s64",
      "userId": "06629147635963609455"
     },
     "user_tz": -330
    },
    "id": "-3IbomL8dMTi"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sys\n",
    "import os \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# exctract word2vec vectors\n",
    "# https://github.com/explosion/spaCy/issues/1721\n",
    "# http://landinghub.visualstudio.com/visual-cpp-build-tools\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is importing several libraries that are commonly used in data analysis and natural language processing. Here is an explanation of each line:\n",
    "\n",
    "1. `import pandas as pd`: This line imports the pandas library and gives it the alias `pd`. Pandas is a library for data manipulation and analysis.\n",
    "2. `import matplotlib.pyplot as plt`: This line imports the `pyplot` module from the `matplotlib` library and gives it the alias `plt`. Matplotlib is a library for creating visualizations.\n",
    "3. `import re`: This line imports the `re` module, which provides regular expression matching operations.\n",
    "4. `import time`: This line imports the `time` module, which provides various time-related functions.\n",
    "5. `import warnings`: This line imports the `warnings` module, which allows you to issue warning messages.\n",
    "6. `import numpy as np`: This line imports the numpy library and gives it the alias `np`. Numpy is a library for working with arrays of data.\n",
    "7. `from nltk.corpus import stopwords`: This line imports the `stopwords` corpus from the Natural Language Toolkit (nltk) library. Stopwords are common words that are often removed from text data when processing natural language.\n",
    "8. `from sklearn.preprocessing import normalize`: This line imports the `normalize` function from the scikit-learn library's preprocessing module. This function is used to scale input vectors individually to unit norm.\n",
    "9. `from sklearn.feature_extraction.text import CountVectorizer`: This line imports the `CountVectorizer` class from scikit-learn's feature_extraction.text module. This class can be used to convert a collection of text documents into a matrix of token counts.\n",
    "10. `from sklearn.feature_extraction.text import TfidfVectorizer`: This line imports the `TfidfVectorizer` class from scikit-learn's feature_extraction.text module. This class can be used to convert a collection of text documents into a matrix of TF-IDF features.\n",
    "11. `warnings.filterwarnings(\"ignore\")`: This line sets the warnings filter to ignore all warnings.\n",
    "12-14. These lines import the sys, os, and pandas libraries again, but without giving them aliases.\n",
    "15. `import numpy as np`: This line imports the numpy library again, but this time with an alias of `np`.\n",
    "16. `from tqdm import tqdm`: This line imports the tqdm function from the tqdm library. Tqdm is a library for creating progress bars in Python loops and iterators.\n",
    "17-19: These lines are comments that provide links to resources related to spaCy and Visual C++ Build Tools.\n",
    "20. `import spacy`: This line imports the spaCy library, which is used for natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "j5XNgVyLdMT7"
   },
   "outputs": [],
   "source": [
    "# avoid decoding problems\n",
    "df = pd.read_csv(\"train.csv\")\n",
    " \n",
    "# encode questions to unicode\n",
    "# https://stackoverflow.com/a/6812069\n",
    "# ----------------- python 2 ---------------------\n",
    "# df['question1'] = df['question1'].apply(lambda x: unicode(str(x),\"utf-8\"))\n",
    "# df['question2'] = df['question2'].apply(lambda x: unicode(str(x),\"utf-8\"))\n",
    "# ----------------- python 3 ---------------------\n",
    "df['question1'] = df['question1'].apply(lambda x: str(x))\n",
    "df['question2'] = df['question2'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an explanation of each line of the code you provided:\n",
    "\n",
    "1. `# avoid decoding problems`: This is a comment that explains the purpose of the following lines of code.\n",
    "2. `df = pd.read_csv(\"train.csv\")`: This line reads a CSV file named \"train.csv\" into a pandas DataFrame called `df`.\n",
    "3. `# encode questions to unicode`: This is a comment that explains the purpose of the following lines of code.\n",
    "4. `# https://stackoverflow.com/a/6812069`: This is a comment that provides a link to a Stack Overflow post related to encoding text in Python.\n",
    "5. `# ----------------- python 2 ---------------------`: This is a comment that indicates that the following lines of code are specific to Python 2.\n",
    "6. `# df['question1'] = df['question1'].apply(lambda x: unicode(str(x),\"utf-8\"))`: This line, which is commented out, shows how the values in the `question1` column of the DataFrame would be encoded as unicode strings in Python 2 using the `unicode` function.\n",
    "7. `# df['question2'] = df['question2'].apply(lambda x: unicode(str(x),\"utf-8\"))`: This line, which is commented out, shows how the values in the `question2` column of the DataFrame would be encoded as unicode strings in Python 2 using the `unicode` function.\n",
    "8. `# ----------------- python 3 ---------------------`: This is a comment that indicates that the following lines of code are specific to Python 3.\n",
    "9. `df['question1'] = df['question1'].apply(lambda x: str(x))`: This line encodes the values in the `question1` column of the DataFrame as strings using the `str` function. The `apply` method is used to apply the lambda function to each element in the specified column of the DataFrame.\n",
    "10. `df['question2'] = df['question2'].apply(lambda x: str(x))`: This line encodes the values in the `question2` column of the DataFrame as strings using the `str` function. The `apply` method is used to apply the lambda function to each element in the specified column of the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HbiMFpgRdMUJ",
    "outputId": "21c00698-7f2a-4ce4-e665-f7a2feaab6fa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RU3HqJXwdMUj"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# merge texts\n",
    "questions = list(df['question1']) + list(df['question2'])\n",
    "\n",
    "tfidf = TfidfVectorizer(lowercase=False, )\n",
    "tfidf.fit_transform(questions)\n",
    "\n",
    "# dict key:word and value:tf-idf score\n",
    "word2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is using the TfidfVectorizer class from scikit-learn's feature_extraction.text module to compute the TF-IDF scores for the words in two columns of a DataFrame. Here is an explanation of each line:\n",
    "\n",
    "1. `from sklearn.feature_extraction.text import TfidfVectorizer`: This line imports the `TfidfVectorizer` class from scikit-learn's feature_extraction.text module. This class can be used to convert a collection of text documents into a matrix of TF-IDF features.\n",
    "2. `from sklearn.feature_extraction.text import CountVectorizer`: This line imports the `CountVectorizer` class from scikit-learn's feature_extraction.text module. This class can be used to convert a collection of text documents into a matrix of token counts.\n",
    "3. `# merge texts`: This is a comment that explains the purpose of the following line of code.\n",
    "4. `questions = list(df['question1']) + list(df['question2'])`: This line creates a new list called `questions` that contains the values from the `question1` and `question2` columns of the DataFrame `df`.\n",
    "5. `tfidf = TfidfVectorizer(lowercase=False, )`: This line creates an instance of the TfidfVectorizer class with the `lowercase` parameter set to `False`.\n",
    "6. `tfidf.fit_transform(questions)`: This line fits the TfidfVectorizer to the data in the `questions` list and transforms it into a matrix of TF-IDF features.\n",
    "7. `# dict key:word and value:tf-idf score`: This is a comment that explains the purpose of the following line of code.\n",
    "8. `word2tfidf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))`: This line creates a dictionary called `word2tfidf` that maps words to their corresponding TF-IDF scores. The keys of the dictionary are obtained by calling the `get_feature_names` method on the TfidfVectorizer instance, and the values are obtained from its `idf_` attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "2JKI2yT4dMUv"
   },
   "source": [
    "- After we find TF-IDF scores, we convert each question to a weighted average of word2vec vectors by these scores.\n",
    "- here we use a pre-trained GLOVE model which comes free with \"Spacy\".  https://spacy.io/usage/vectors-similarity\n",
    "- It is trained on Wikipedia and therefore, it is stronger in terms of word semantics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 108164,
     "status": "ok",
     "timestamp": 1644584896974,
     "user": {
      "displayName": "Applied AI Course",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjSpfk3evOnQu4WaAr65Jsk94o0T_MicRlfD3-hAVs=s64",
      "userId": "06629147635963609455"
     },
     "user_tz": -330
    },
    "id": "sGpwzISi3pyU",
    "outputId": "102981e6-82cf-4b75-8017-ff12b094ad3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl (587.7 MB)\n",
      "     ---------------------------------------- 587.7/587.7 MB ? eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from en-core-web-lg==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.64.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.27.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.21.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.11.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (65.5.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (21.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.3.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 17:13:48.193148: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\n",
      "2023-04-04 17:13:48.193197: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.5.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.5.0) (3.5.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.21.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (65.5.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.11.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2021.10.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\sagar\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.5.0\n",
      "\u001b[38;5;3m[!] As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use\n",
      "the full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 17:26:18.044604: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\n",
      "2023-04-04 17:26:18.044640: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en_core_web_md\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 404290/404290 [40:05<00:00, 168.10it/s]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "vecs1 = []\n",
    "\n",
    "for qu1 in tqdm(list(df['question1'])):\n",
    "    doc1 = nlp(qu1) \n",
    "    mean_vec1 = np.zeros([len(doc1), len(doc1[0].vector)])\n",
    "    for word1 in doc1:\n",
    "        vec1 = word1.vector\n",
    "        try:\n",
    "            idf = word2tfidf[str(word1)]\n",
    "        except:\n",
    "            idf = 0\n",
    "        mean_vec1 += vec1 * idf\n",
    "    mean_vec1 = mean_vec1.mean(axis=0)\n",
    "    vecs1.append(mean_vec1)\n",
    "\n",
    "df['q1_feats_m'] = list(vecs1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is using the spaCy library to compute the mean word2vec vector for each question in the `question1` column of a DataFrame called `df`. Here is an explanation of each line:\n",
    "\n",
    "1. `# en_vectors_web_lg, which includes over 1 million unique vectors.`: This is a comment that provides some information about the `en_vectors_web_lg` model.\n",
    "2. `nlp = spacy.load('en_core_web_lg')`: This line loads the `en_core_web_lg` model from spaCy and assigns it to the variable `nlp`.\n",
    "3. `vecs1 = []`: This line creates an empty list called `vecs1`.\n",
    "4. `# # https://github.com/noamraph/tqdm`: This is a comment that provides a link to the GitHub repository for the tqdm library.\n",
    "5. `## tqdm is used to print the progrss bar`: This is a comment that explains the purpose of using the tqdm library in the following lines of code.\n",
    "6. `for qu1 in tqdm(list(df['question1'])):`: This line starts a for loop that iterates over the values in the `question1` column of the DataFrame `df`. The `tqdm` function is used to create a progress bar for the loop.\n",
    "7. `doc1 = nlp(qu1)`: This line processes each question using the spaCy model and assigns the resulting Doc object to the variable `doc1`.\n",
    "8. `# 384 is the number of dimensions of vectors`: This is a comment that provides some information about the size of the word vectors.\n",
    "9. `mean_vec1 = np.zeros([len(doc1), len(doc1[0].vector)])`: This line creates an array of zeros with shape `(len(doc1), len(doc1[0].vector))` and assigns it to the variable `mean_vec1`.\n",
    "10. `for word1 in doc1:`: This line starts a nested for loop that iterates over each token in the Doc object.\n",
    "11. `# word2vec`: This is a comment that indicates that the following lines of code are related to computing word2vec vectors.\n",
    "12. `vec1 = word1.vector`: This line extracts the word vector for each token and assigns it to the variable `vec1`.\n",
    "13. `# fetch df score`: This is a comment that explains the purpose of the following lines of code.\n",
    "14. `try:`: This line starts a try block that attempts to retrieve the IDF score for each token from a dictionary called `word2tfidf`.\n",
    "15. `idf = word2tfidf[str(word1)]`: This line retrieves the IDF score for each token from the dictionary and assigns it to the variable `idf`.\n",
    "16. `except:`: This line starts an except block that handles cases where a token is not found in the dictionary.\n",
    "17. `idf = 0`: This line sets the value of `idf` to 0 if a token is not found in the dictionary.\n",
    "18. `# compute final vec`: This is a comment that explains the purpose of the following line of code.\n",
    "19. `mean_vec1 += vec1 * idf`: This line updates the value of `mean_vec1` by adding the product of each token's word vector and its IDF score.\n",
    "20. `mean_vec1 = mean_vec1.mean(axis=0)`: After all tokens have been processed, this line computes the mean of all word vectors along axis 0 and assigns it to `mean_vec1`.\n",
    "21. `vecs1.append(mean_vec1)`: This line appends each mean vector to the list called `vecs1`.\n",
    "22. `df['q1_feats_m'] = list(vecs1)`: After all questions have been processed, this line adds a new column called `'q1_feats_m'` to the DataFrame and sets its values to be equal to those in the list called `vecs1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "62GEF-RbdMVB",
    "outputId": "60a4f5f8-5582-4886-befd-2ab6ed99c753"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 404290/404290 [37:37<00:00, 179.05it/s]\n"
     ]
    }
   ],
   "source": [
    "vecs2 = []\n",
    "for qu2 in tqdm(list(df['question2'])):\n",
    "    doc2 = nlp(qu2) \n",
    "    mean_vec2 = np.zeros([len(doc1), len(doc2[0].vector)])\n",
    "    for word2 in doc2:\n",
    "        # word2vec\n",
    "        vec2 = word2.vector\n",
    "        # fetch df score\n",
    "        try:\n",
    "            idf = word2tfidf[str(word2)]\n",
    "        except:\n",
    "            #print word\n",
    "            idf = 0\n",
    "        # compute final vec\n",
    "        mean_vec2 += vec2 * idf\n",
    "    mean_vec2 = mean_vec2.mean(axis=0)\n",
    "    vecs2.append(mean_vec2)\n",
    "df['q2_feats_m'] = list(vecs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code processes a column of text data named `question2` in a DataFrame `df`. Here's a line-by-line explanation:\n",
    "\n",
    "1. `vecs2 = []`: Initializes an empty list `vecs2` to store the results.\n",
    "2. `for qu2 in tqdm(list(df['question2'])):`: Iterates over each element in the `question2` column of the DataFrame `df`. The `tqdm` function is used to display a progress bar.\n",
    "3. `doc2 = nlp(qu2)`: Processes the text data using the `nlp` function from the spaCy library.\n",
    "4. `mean_vec1 = np.zeros([len(doc1), len(doc2[0].vector)])`: Initializes an array of zeros with shape `(len(doc1), len(doc2[0].vector))` using the NumPy library. The purpose of this array is not clear from the provided code.\n",
    "5. `for word2 in doc2:`: Iterates over each token in the processed text data.\n",
    "6. `vec2 = word2.vector`: Retrieves the word vector for the current token.\n",
    "7. `try: idf = word2tfidf[str(word2)]`: Attempts to retrieve the inverse document frequency (IDF) score for the current token from a dictionary named `word2tfidf`.\n",
    "8. `except: idf = 0`: If the IDF score is not found in the dictionary, it is set to 0.\n",
    "9. `mean_vec2 += vec2 * idf`: Multiplies the word vector by its IDF score and adds it to a running total.\n",
    "10. `mean_vec2 = mean_vec2.mean(axis=0)`: Computes the mean of the accumulated word vectors along axis 0.\n",
    "11. `vecs2.append(mean_vec2)`: Appends the computed mean vector to the list `vecs2`.\n",
    "12. `df['q2_feats_m'] = list(vecs2)`: Adds a new column named `q2_feats_m` to the DataFrame and assigns it the values in the list `vecs2`.\n",
    "\n",
    "This code appears to compute a weighted average of word vectors for each element in the `question2` column of a DataFrame, where the weights are determined by the IDF scores of the words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a38GBlGWdMVQ"
   },
   "outputs": [],
   "source": [
    "#prepro_features_train.csv (Simple Preprocessing Feartures)\n",
    "#nlp_features_train.csv (NLP Features)\n",
    "if os.path.isfile('nlp_features_train.csv'):\n",
    "    dfnlp = pd.read_csv(\"nlp_features_train.csv\",encoding='latin-1')\n",
    "else:\n",
    "    print(\"download nlp_features_train.csv from drive or run previous notebook\")\n",
    "\n",
    "if os.path.isfile('df_fe_without_preprocessing_train.csv'):\n",
    "    dfppro = pd.read_csv(\"df_fe_without_preprocessing_train.csv\",encoding='latin-1')\n",
    "else:\n",
    "    print(\"download df_fe_without_preprocessing_train.csv from drive or run previous notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code checks if two files, `nlp_features_train.csv` and `df_fe_without_preprocessing_train.csv`, exist in the current working directory. If they do, the code reads them into two DataFrames `dfnlp` and `dfppro` using the `pandas` library. If either file is not found, a message is printed instructing the user to download it or run a previous notebook. Here's a line-by-line explanation:\n",
    "\n",
    "1. `#prepro_features_train.csv (Simple Preprocessing Feartures)`: A comment indicating that the file `prepro_features_train.csv` contains simple preprocessing features.\n",
    "2. `#nlp_features_train.csv (NLP Features)`: A comment indicating that the file `nlp_features_train.csv` contains NLP features.\n",
    "3. `if os.path.isfile('nlp_features_train.csv'):`: Checks if the file `nlp_features_train.csv` exists in the current working directory using the `isfile` function from the `os.path` module.\n",
    "4. `dfnlp = pd.read_csv(\"nlp_features_train.csv\",encoding='latin-1')`: If the file exists, it is read into a DataFrame named `dfnlp` using the `read_csv` function from the `pandas` library. The file is assumed to be encoded in `'latin-1'`.\n",
    "5. `else: print(\"download nlp_features_train.csv from drive or run previous notebook\")`: If the file does not exist, a message is printed instructing the user to download it or run a previous notebook.\n",
    "6. `if os.path.isfile('df_fe_without_preprocessing_train.csv'):`: Checks if the file `df_fe_without_preprocessing_train.csv` exists in the current working directory using the same method as before.\n",
    "7. `dfppro = pd.read_csv(\"df_fe_without_preprocessing_train.csv\",encoding='latin-1')`: If the file exists, it is read into a DataFrame named `dfppro` using the same method as before.\n",
    "8. `else: print(\"download df_fe_without_preprocessing_train.csv from drive or run previous notebook\")`: If the file does not exist, a message is printed instructing the user to download it or run a previous notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "apdRa1kndMVb"
   },
   "outputs": [],
   "source": [
    "df1 = dfnlp.drop(['qid1','qid2','question1','question2'],axis=1)\n",
    "df2 = dfppro.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\n",
    "df3 = df.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)\n",
    "df3_q1 = pd.DataFrame(df3.q1_feats_m.values.tolist(), index= df3.index)\n",
    "df3_q2 = pd.DataFrame(df3.q2_feats_m.values.tolist(), index= df3.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs several operations on three DataFrames `dfnlp`, `dfppro`, and `df`. Here's a line-by-line explanation:\n",
    "\n",
    "1. `df1 = dfnlp.drop(['qid1','qid2','question1','question2'],axis=1)`: Creates a new DataFrame `df1` by dropping the columns `['qid1','qid2','question1','question2']` from the DataFrame `dfnlp` using the `drop` method. The `axis=1` parameter indicates that columns are being dropped.\n",
    "2. `df2 = dfppro.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)`: Creates a new DataFrame `df2` by dropping the columns `['qid1','qid2','question1','question2','is_duplicate']` from the DataFrame `dfppro` using the same method as before.\n",
    "3. `df3 = df.drop(['qid1','qid2','question1','question2','is_duplicate'],axis=1)`: Creates a new DataFrame `df3` by dropping the columns `['qid1','qid2','question1','question2','is_duplicate']` from the DataFrame `df` using the same method as before.\n",
    "4. `df3_q1 = pd.DataFrame(df3.q1_feats_m.values.tolist(), index= df3.index)`: Creates a new DataFrame `df3_q1` by converting the values in the `q1_feats_m` column of the DataFrame `df3` to a list of lists using the `tolist` method and passing it to the `DataFrame` constructor from the `pandas` library. The index of the new DataFrame is set to be the same as that of the DataFrame `df3`.\n",
    "5. `df3_q2 = pd.DataFrame(df3.q2_feats_m.values.tolist(), index= df3.index)`: Creates a new DataFrame `df3_q2` using the same method as before, but with the values in the `q2_feats_m` column of the DataFrame `df3`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xzWAqGegdMVp",
    "outputId": "2f88eeda-244f-4bbb-a51c-a8680fe8fb92"
   },
   "outputs": [],
   "source": [
    "# dataframe of nlp features\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N4DQnDtndMV4",
    "outputId": "2e288eed-e8fa-4ec3-a9b9-4e4daba52fc1"
   },
   "outputs": [],
   "source": [
    "# data before preprocessing \n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1YIPtTwdMWC",
    "outputId": "510f4c73-0706-4633-d706-e0d348ebfa71"
   },
   "outputs": [],
   "source": [
    "# Questions 1 tfidf weighted word2vec\n",
    "df3_q1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUMdkJTNdMWL",
    "outputId": "69e3e256-cbb8-4fe2-aaf2-9088c3868b29"
   },
   "outputs": [],
   "source": [
    "# Questions 2 tfidf weighted word2vec\n",
    "df3_q2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ozz83vh4dMWU",
    "outputId": "e5b30f77-2849-4b08-9949-0912ec0db418"
   },
   "outputs": [],
   "source": [
    "print(\"Number of features in nlp dataframe :\", df1.shape[1])\n",
    "print(\"Number of features in preprocessed dataframe :\", df2.shape[1])\n",
    "print(\"Number of features in question1 w2v  dataframe :\", df3_q1.shape[1])\n",
    "print(\"Number of features in question2 w2v  dataframe :\", df3_q2.shape[1])\n",
    "print(\"Number of features in final dataframe  :\", df1.shape[1]+df2.shape[1]+df3_q1.shape[1]+df3_q2.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HmfZ5Q1zdMWl"
   },
   "outputs": [],
   "source": [
    "# storing the final features to csv file\n",
    "if not os.path.isfile('final_features.csv'):\n",
    "    df3_q1['id']=df1['id']\n",
    "    df3_q2['id']=df1['id']\n",
    "    df1  = df1.merge(df2, on='id',how='left')\n",
    "    df2  = df3_q1.merge(df3_q2, on='id',how='left')\n",
    "    result  = df1.merge(df2, on='id',how='left')\n",
    "    result.to_csv('final_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code checks if a file named `final_features.csv` exists in the current working directory. If it does not, the code performs several operations to merge the DataFrames `df1`, `df2`, `df3_q1`, and `df3_q2` into a single DataFrame `result` and writes it to a CSV file named `final_features.csv`. Here's a line-by-line explanation:\n",
    "\n",
    "1. `# storing the final features to csv file`: A comment indicating that the purpose of the code is to store the final features to a CSV file.\n",
    "2. `if not os.path.isfile('final_features.csv'):`: Checks if the file `final_features.csv` does not exist in the current working directory using the `isfile` function from the `os.path` module.\n",
    "3. `df3_q1['id']=df1['id']`: Adds a new column named `id` to the DataFrame `df3_q1` and assigns it the values from the `id` column of the DataFrame `df1`.\n",
    "4. `df3_q2['id']=df1['id']`: Adds a new column named `id` to the DataFrame `df3_q2` and assigns it the values from the `id` column of the DataFrame `df1`.\n",
    "5. `df1  = df1.merge(df2, on='id',how='left')`: Merges the DataFrames `df1` and `df2` into a single DataFrame using the `merge` method from the `pandas` library. The merge is performed on the `id` column and uses a left join.\n",
    "6. `df2  = df3_q1.merge(df3_q2, on='id',how='left')`: Merges the DataFrames `df3_q1` and `df3_q2` into a single DataFrame using the same method as before.\n",
    "7. `result  = df1.merge(df2, on='id',how='left')`: Merges the resulting DataFrames from steps 5 and 6 into a single DataFrame named `result` using the same method as before.\n",
    "8. `result.to_csv('final_features.csv')`: Writes the resulting DataFrame to a CSV file named `final_features.csv` using the `to_csv` method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3.Q_Mean_W2V.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
